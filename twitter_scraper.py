#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import logging
from datetime import datetime
import requests
from dotenv import load_dotenv

class TwitterScraper:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.base_url = "https://api.twitter.com/2"
        self.bearer_token = os.getenv('TWITTER_BEARER_TOKEN')
        
    def get_user_id(self, username):
        """è·å–ç”¨æˆ·ID"""
        url = f"{self.base_url}/users/by/username/{username}"
        headers = {
            "Authorization": f"Bearer {self.bearer_token}",
            "User-Agent": "v2UserLookupPython"
        }
        
        try:
            response = requests.get(url, headers=headers)
            if response.status_code == 200:
                return response.json()['data']['id']
            else:
                self.logger.error(f"è·å–ç”¨æˆ·IDå¤±è´¥: {response.text}")
                return None
        except Exception as e:
            self.logger.error(f"è·å–ç”¨æˆ·IDæ—¶å‡ºé”™: {str(e)}")
            return None
            
    def get_user_tweets(self, user_id, max_results=10):
        """è·å–ç”¨æˆ·çš„æ¨æ–‡"""
        url = f"{self.base_url}/users/{user_id}/tweets"
        params = {
            "max_results": max_results,
            "tweet.fields": "created_at,public_metrics,entities,attachments",
            "expansions": "attachments.media_keys",
            "media.fields": "url,preview_image_url"
        }
        headers = {
            "Authorization": f"Bearer {self.bearer_token}",
            "User-Agent": "v2TweetLookupPython"
        }
        
        try:
            response = requests.get(url, headers=headers, params=params)
            if response.status_code == 200:
                return response.json()
            else:
                self.logger.error(f"è·å–æ¨æ–‡å¤±è´¥: {response.text}")
                return None
        except Exception as e:
            self.logger.error(f"è·å–æ¨æ–‡æ—¶å‡ºé”™: {str(e)}")
            return None
            
    def process_tweets(self, tweets_data):
        """å¤„ç†æ¨æ–‡æ•°æ®"""
        if not tweets_data or 'data' not in tweets_data:
            return []
            
        processed_tweets = []
        media_lookup = {}
        
        # åˆ›å»ºåª’ä½“æŸ¥æ‰¾è¡¨
        if 'includes' in tweets_data and 'media' in tweets_data['includes']:
            for media in tweets_data['includes']['media']:
                media_lookup[media['media_key']] = media.get('url') or media.get('preview_image_url')
        
        for tweet in tweets_data['data']:
            try:
                # è·å–åª’ä½“URL
                media_urls = []
                if 'attachments' in tweet and 'media_keys' in tweet['attachments']:
                    for media_key in tweet['attachments']['media_keys']:
                        if media_key in media_lookup:
                            media_urls.append(media_lookup[media_key])
                
                # è·å–é“¾æ¥
                urls = []
                if 'entities' in tweet and 'urls' in tweet['entities']:
                    urls = [url['expanded_url'] for url in tweet['entities']['urls']]
                
                processed_tweet = {
                    'text': tweet['text'],
                    'created_at': tweet['created_at'],
                    'tweet_id': tweet['id'],
                    'likes': tweet['public_metrics']['like_count'],
                    'retweets': tweet['public_metrics']['retweet_count'],
                    'replies': tweet['public_metrics']['reply_count'],
                    'quotes': tweet['public_metrics']['quote_count'],
                    'media_urls': media_urls,
                    'urls': urls,
                    'platform': 'Twitter',
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
                processed_tweets.append(processed_tweet)
                
            except Exception as e:
                self.logger.error(f"å¤„ç†æ¨æ–‡æ—¶å‡ºé”™: {str(e)}")
                continue
                
        return processed_tweets
            
    def save_tweets_to_json(self, tweets, output_file='twitter_posts.json'):
        """ä¿å­˜æ¨æ–‡ä¿¡æ¯åˆ°JSONæ–‡ä»¶"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(tweets, f, ensure_ascii=False, indent=2)
            self.logger.info(f"å·²ä¿å­˜æ¨æ–‡ä¿¡æ¯åˆ° {output_file}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜JSONæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            
    def generate_html(self, tweets, output_file='twitter_posts.html'):
        """ç”Ÿæˆæ¨æ–‡åˆ—è¡¨çš„HTMLå†…å®¹"""
        try:
            html_content = '\n'.join([
                '''
                <div class="tweet-card">
                    <div class="tweet-info">
                        <span class="platform-tag platform-twitter">Twitter</span>
                        <p class="tweet-text">{text}</p>
                        <div class="tweet-media">
                            {media_html}
                        </div>
                        <div class="tweet-meta">
                            <span>â¤ï¸ {likes}</span>
                            <span>ğŸ”„ {retweets}</span>
                            <span>ğŸ’¬ {replies}</span>
                            <span>ğŸ“ {quotes}</span>
                        </div>
                        <p class="tweet-time">å‘å¸ƒæ—¶é—´: {created_at}</p>
                        <div class="tweet-links">
                            {links_html}
                        </div>
                    </div>
                </div>
                '''.format(
                    text=tweet['text'],
                    likes=tweet['likes'],
                    retweets=tweet['retweets'],
                    replies=tweet['replies'],
                    quotes=tweet['quotes'],
                    created_at=tweet['created_at'],
                    media_html='\n'.join([f'<img src="{url}" alt="Media">' for url in tweet['media_urls']]),
                    links_html='\n'.join([f'<a href="{url}" target="_blank">{url}</a>' for url in tweet['urls']])
                ) for tweet in tweets
            ])
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            self.logger.info(f"å·²ç”ŸæˆHTMLå†…å®¹: {output_file}")
            
        except Exception as e:
            self.logger.error(f"ç”ŸæˆHTMLå†…å®¹æ—¶å‡ºé”™: {str(e)}")
            
    def run_scraper(self):
        """è¿è¡Œçˆ¬è™«"""
        try:
            # åŠ è½½ç¯å¢ƒå˜é‡
            load_dotenv()
            
            if not self.bearer_token:
                print("é”™è¯¯ï¼šæœªåœ¨.envæ–‡ä»¶ä¸­æ‰¾åˆ°TWITTER_BEARER_TOKENé…ç½®")
                return
            
            # è·å–ç”¨æˆ·ID
            username = "op7418"
            user_id = self.get_user_id(username)
            
            if not user_id:
                print(f"æœªèƒ½è·å–åˆ°ç”¨æˆ· {username} çš„ID")
                return
            
            # è·å–ç”¨æˆ·æ¨æ–‡
            print(f"\nå¼€å§‹è·å–ç”¨æˆ· {username} çš„æ¨æ–‡")
            tweets_data = self.get_user_tweets(user_id)
            
            if tweets_data:
                # å¤„ç†æ¨æ–‡æ•°æ®
                processed_tweets = self.process_tweets(tweets_data)
                
                if processed_tweets:
                    # ä¿å­˜æ¨æ–‡ä¿¡æ¯
                    self.save_tweets_to_json(processed_tweets)
                    self.generate_html(processed_tweets)
                    print(f"\næ€»è®¡è·å–åˆ° {len(processed_tweets)} æ¡æ¨æ–‡")
                else:
                    print("\næœªèƒ½è·å–åˆ°ä»»ä½•æ¨æ–‡ä¿¡æ¯")
            else:
                print(f"\næœªèƒ½è·å–åˆ°ç”¨æˆ· {username} çš„æ¨æ–‡")
            
        except Exception as e:
            self.logger.error(f"è¿è¡Œçˆ¬è™«æ—¶å‡ºé”™: {str(e)}")
            raise

def main():
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('twitter_scraper.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    scraper = TwitterScraper()
    scraper.run_scraper()

if __name__ == "__main__":
    main()
